{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs3jGWzOYUO0"
   },
   "source": [
    "# Autoregressives Modell auf MNIST mit Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:21.079717Z",
     "start_time": "2022-01-28T14:38:17.897406Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOx0_M5nYUO7",
    "outputId": "62f57278-2414-4f32-c212-ec348d250ef5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import random\n",
    "\n",
    "#Kleine Module von Lukas Rinder https://github.com/LukasRinder/normalizing-flows:\n",
    "from LukasRinder.LukasRinder import load_and_preprocess_mnist\n",
    "from LukasRinder.LukasRinder import Made\n",
    "from LukasRinder.LukasRinder import train_density_estimation, nll\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dieser Abschnitt stammt von https://www.tensorflow.org/tutorials/generative/autoencoder Zugriff: 28.01.2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten Laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:21.478798Z",
     "start_time": "2022-01-28T14:38:21.082711Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "print (x_train.shape)\n",
    "print (x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder implementieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:21.509950Z",
     "start_time": "2022-01-28T14:38:21.480783Z"
    }
   },
   "outputs": [],
   "source": [
    "latent_dim = 64 \n",
    "\n",
    "class Autoencoder(Model):\n",
    "  def __init__(self, latent_dim):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.latent_dim = latent_dim   \n",
    "    self.encoder = tf.keras.Sequential([\n",
    "      layers.Flatten(),\n",
    "      layers.Dense(latent_dim, activation='relu'),\n",
    "    ])\n",
    "    self.decoder = tf.keras.Sequential([\n",
    "      layers.Dense(784, activation='sigmoid'),\n",
    "      layers.Reshape((28, 28))\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    encoded = self.encoder(x)\n",
    "    decoded = self.decoder(encoded)\n",
    "    return decoded\n",
    "\n",
    "autoencoder = Autoencoder(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:21.525254Z",
     "start_time": "2022-01-28T14:38:21.513956Z"
    }
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer='adam', loss=losses.MeanSquaredError())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:46.347411Z",
     "start_time": "2022-01-28T14:38:21.527979Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=10,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autoencoder testen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:46.410512Z",
     "start_time": "2022-01-28T14:38:46.350376Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_imgs = autoencoder.encoder(x_test).numpy()\n",
    "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:47.295664Z",
     "start_time": "2022-01-28T14:38:46.411540Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "  # display original\n",
    "  ax = plt.subplot(2, n, i + 1)\n",
    "  plt.imshow(x_test[i])\n",
    "  plt.title(\"original\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "\n",
    "  # display reconstruction\n",
    "  ax = plt.subplot(2, n, i + 1 + n)\n",
    "  plt.imshow(decoded_imgs[i])\n",
    "  plt.title(\"reconstructed\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daten encoden und vorbereiten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:47.357401Z",
     "start_time": "2022-01-28T14:38:47.297419Z"
    }
   },
   "outputs": [],
   "source": [
    "low_dim_train = autoencoder.encoder(x_train[:50000]).numpy()\n",
    "low_dim_val = autoencoder.encoder(x_train[50000:]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:47.625741Z",
     "start_time": "2022-01-28T14:38:47.359226Z"
    }
   },
   "outputs": [],
   "source": [
    "maximum = -1000000\n",
    "for i in low_dim_train:\n",
    "    if max(i) > maximum:\n",
    "        maximum = max(i)\n",
    "for j in low_dim_val:\n",
    "    if max(j) > maximum:\n",
    "        maximum = max(j)\n",
    "print(maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:47.894719Z",
     "start_time": "2022-01-28T14:38:47.628506Z"
    }
   },
   "outputs": [],
   "source": [
    "minimum = 1000000\n",
    "for i in low_dim_train:\n",
    "    if min(i) < minimum:\n",
    "        minimum = min(i)\n",
    "for j in low_dim_val:\n",
    "    if min(j) < minimum:\n",
    "        minimum = min(j)\n",
    "print(minimum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:47.910617Z",
     "start_time": "2022-01-28T14:38:47.896641Z"
    }
   },
   "outputs": [],
   "source": [
    "low_dim_train_scale = tf.cast(low_dim_train / 29.337349, tf.float32)\n",
    "low_dim_val_scale = tf.cast(low_dim_val / 29.337349, tf.float32)\n",
    "low_dim_train_scale = tf.reshape(low_dim_train_scale, (low_dim_train_scale.shape[0], 8, 8))\n",
    "low_dim_val_scale = tf.reshape(low_dim_val_scale, (low_dim_val_scale.shape[0], 8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:47.925600Z",
     "start_time": "2022-01-28T14:38:47.912566Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_s = 128\n",
    "\n",
    "shuffled_train = tf.data.Dataset.from_tensor_slices(low_dim_train_scale).shuffle(1000)\n",
    "batched_train = shuffled_train.batch(batch_s)\n",
    "batched_val = tf.data.Dataset.from_tensor_slices(low_dim_val_scale).batch(batch_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisierung der 8x8 Bilder aus SpaÃŸ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:38:48.066043Z",
     "start_time": "2022-01-28T14:38:47.927598Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(next(iter(batched_train))[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion wie in anderen Beispielen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:40:42.684618Z",
     "start_time": "2022-01-28T14:40:42.666633Z"
    }
   },
   "outputs": [],
   "source": [
    "def AutoregressiveFlow(dimension, layers, hidden_shape=[512, 512], activation=\"relu\", inverse=False):\n",
    "    base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(shape=dimension, dtype=tf.float32))\n",
    "    bijectors = []\n",
    "    permutation = tf.cast(np.concatenate((np.arange(dimension/2,dimension),np.arange(0,dimension/2))), tf.int32)\n",
    "    params=0\n",
    "    if inverse:\n",
    "        for i in range(layers):\n",
    "            bijectors.append(tfb.Invert(tfb.MaskedAutoregressiveFlow(\n",
    "                shift_and_log_scale_fn = Made(params=2, hidden_units=hidden_shape, activation=activation))))\n",
    "            bijectors.append(tfb.Permute(permutation=permutation))\n",
    "    else:\n",
    "        for i in range(layers):\n",
    "            bijectors.append(tfb.MaskedAutoregressiveFlow(\n",
    "                shift_and_log_scale_fn = Made(params=2, hidden_units=hidden_shape, activation=activation)))\n",
    "            bijectors.append(tfb.Permute(permutation=permutation))\n",
    "        \n",
    "    \n",
    "    bijectors.append(tfb.Reshape(event_shape_out=(int(np.sqrt(dimension)),int(np.sqrt(dimension))),\n",
    "                                 event_shape_in=(dimension,)))\n",
    "    bijector = tfb.Chain(bijectors=list(reversed(bijectors)))\n",
    "    \n",
    "    masked_auto_flow = tfd.TransformedDistribution(distribution=base_dist, bijector=bijector)\n",
    "    masked_auto_flow.log_prob(tf.reshape(base_dist.sample(), (8, 8)))\n",
    "    for theta in masked_auto_flow.trainable_variables:\n",
    "        params += np.prod(theta.shape)\n",
    "    print(\"trainable parameters:\", params)\n",
    "    return masked_auto_flow, base_dist, bijectors, bijector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter festlegen und einen Namen fÃ¼r die Checkpoints festlegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:40:54.591832Z",
     "start_time": "2022-01-28T14:40:54.583814Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = \"mnist_auto\"\n",
    "layers = 20\n",
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "epochs = 200\n",
    "shape = [128, 128]\n",
    "mnist_trainsize = 50000\n",
    "dimension = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell initialisieren. In diesem Stadium entspricht MAF der Startverteilung bzw. full_bijector der IdentitÃ¤tsabbildung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:40:45.214544Z",
     "start_time": "2022-01-28T14:40:45.035983Z"
    }
   },
   "outputs": [],
   "source": [
    "MAF, base_dist, list_of_bijectors, full_bijector = AutoregressiveFlow(dimension, layers, shape, inverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:40:56.883238Z",
     "start_time": "2022-01-28T14:40:56.877226Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, epochs, end_lr, power=0.5)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:40:59.465100Z",
     "start_time": "2022-01-28T14:40:59.452120Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt_dir = f\"{dataset}/tmp_{layers}\"\n",
    "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt\")\n",
    "\n",
    "ckpt = tf.train.Checkpoint(optimizer=opt, model=MAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:41:02.678484Z",
     "start_time": "2022-01-28T14:41:02.669508Z"
    }
   },
   "outputs": [],
   "source": [
    "def TrainFlow(flow, batched_train, batched_val, epochs, train_size, optimizer, checkpoint, checkpoint_pref):\n",
    "\n",
    "    t_losses, v_losses = [], []\n",
    "    t_start = time.time()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        batched_train.shuffle(buffer_size=train_size, reshuffle_each_iteration=True)\n",
    "        batch_t_losses = []\n",
    "        for batch in batched_train:\n",
    "            batch_loss = train_density_estimation(flow, optimizer, batch)\n",
    "            batch_t_losses.append(batch_loss)\n",
    "        t_loss = tf.reduce_mean(batch_t_losses)\n",
    "\n",
    "        batch_v_losses = []\n",
    "        for batch in batched_val:\n",
    "            batch_loss = nll(flow, batch)\n",
    "            batch_v_losses.append(batch_loss)\n",
    "        v_loss = tf.reduce_mean(batch_v_losses)\n",
    "\n",
    "        t_losses.append(t_loss)\n",
    "        v_losses.append(v_loss)\n",
    "        print(f\"Epoch {i+1}: train loss: {t_loss}, val loss: {v_loss}\")\n",
    "        \n",
    "        if i == 0:\n",
    "            min_v_loss = v_loss\n",
    "            best_epoch = 0\n",
    "        if v_loss < min_v_loss:\n",
    "            min_v_loss = v_loss\n",
    "            best_epoch = i\n",
    "            checkpoint.write(file_prefix=checkpoint_pref)\n",
    "                \n",
    "    print(\"train time:\", time.time() - t_start)\n",
    "    \n",
    "    return t_losses, v_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:41:12.603465Z",
     "start_time": "2022-01-28T14:41:05.164713Z"
    }
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses = TrainFlow(MAF, batched_train, batched_val, \n",
    "                                     epochs, mnist_trainsize, opt, ckpt, ckpt_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:41:15.910987Z",
     "start_time": "2022-01-28T14:41:15.770205Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_losses)), train_losses, label=\"train loss\")\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"val loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:41:18.113626Z",
     "start_time": "2022-01-28T14:41:18.041279Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stichproben generieren und durchschnittliche Zeit ausgeben.\n",
    "#### Dazu mÃ¼ssen 8x8 Stichproben der zug. Normalverteilung generiert, dann transformiert und anschlieÃŸend decoded werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:41:26.915612Z",
     "start_time": "2022-01-28T14:41:26.152578Z"
    }
   },
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "examples = tf.reshape(MAF.sample(20)*maximum, (20, 64)).numpy()\n",
    "examples = autoencoder.decoder(examples).numpy()\n",
    "sample_time = time.time() -s_time\n",
    "sample_time = sample_time/20\n",
    "sample_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stichporben visualisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:41:31.869448Z",
     "start_time": "2022-01-28T14:41:31.429443Z"
    }
   },
   "outputs": [],
   "source": [
    "n = 10\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "  ax = plt.subplot(2, n, i + 1 + n)\n",
    "  plt.imshow(examples[i])\n",
    "  plt.title(\"generated\")\n",
    "  plt.gray()\n",
    "  ax.get_xaxis().set_visible(False)\n",
    "  ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "maf_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
