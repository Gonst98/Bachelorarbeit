{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gs3jGWzOYUO0"
   },
   "source": [
    "# Autoregressives Modell mit affinem Transformer auf MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T14:23:24.030526Z",
     "start_time": "2022-01-28T14:23:21.355477Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOx0_M5nYUO7",
    "outputId": "62f57278-2414-4f32-c212-ec348d250ef5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import os\n",
    "import random\n",
    "\n",
    "#Kleine Module von Lukas Rinder https://github.com/LukasRinder/normalizing-flows:\n",
    "from LukasRinder.LukasRinder import load_and_preprocess_mnist\n",
    "from LukasRinder.LukasRinder import Made\n",
    "from LukasRinder.LukasRinder import train_density_estimation, nll\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors\n",
    "\n",
    "tf.random.set_seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nicht alle Versionen von Tensorflow-Probability sind mit alles Python Versionen kompatibel. Tfp steckt noch in der Beta-Version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:16.931156Z",
     "start_time": "2022-01-28T13:03:16.919188Z"
    }
   },
   "outputs": [],
   "source": [
    "print(np.__version__, tf.__version__, tfp.__version__, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST Daten von tf.keras.datasets laden, auf [0,1] skalieren und Batches initialisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.373398Z",
     "start_time": "2022-01-28T13:03:16.933150Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "category = -1\n",
    "batched_train, batched_val, batched_test, _ = load_and_preprocess_mnist(\n",
    "                                                    logit_space=False, batch_size=batch_size, classes=category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.606331Z",
     "start_time": "2022-01-28T13:03:17.375424Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(next(iter(batched_train))[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion, die ein MAF bzw. IAF Modell erzeugt.\n",
    "#### Die Permutation ist hier fest gewählt und vertauscht die ersten 14 Zeilen mit den restlichen 14 als Block. Die einzelnen Transformationen werden mit tfb.Chain verkettet. Hier wird das in umgekehrter Reihenfolge getan, sodass die zuerst implementierte Transformation T1 entspricht (auf dem latenten Raum operiert)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.622311Z",
     "start_time": "2022-01-28T13:03:17.608328Z"
    }
   },
   "outputs": [],
   "source": [
    "def AutoregressiveFlow(dimension, layers, hidden_shape=[512, 512], activation=\"relu\", inverse=False):\n",
    "    base_dist = tfd.MultivariateNormalDiag(loc=tf.zeros(shape=dimension, dtype=tf.float32))\n",
    "    bijectors = []\n",
    "    permutation = tf.cast(np.concatenate((np.arange(dimension/2,dimension),np.arange(0,dimension/2))), tf.int32)\n",
    "    params=0\n",
    "    if inverse:\n",
    "        for i in range(layers):\n",
    "            bijectors.append(tfb.Invert(tfb.MaskedAutoregressiveFlow(\n",
    "                shift_and_log_scale_fn = Made(params=2, hidden_units=hidden_shape, activation=activation))))\n",
    "            bijectors.append(tfb.Permute(permutation=permutation))\n",
    "    else:\n",
    "        for i in range(layers):\n",
    "            bijectors.append(tfb.MaskedAutoregressiveFlow(\n",
    "                shift_and_log_scale_fn = Made(params=2, hidden_units=hidden_shape, activation=activation)))\n",
    "            bijectors.append(tfb.Permute(permutation=permutation))\n",
    "        \n",
    "    \n",
    "    bijectors.append(tfb.Reshape(event_shape_out=(int(np.sqrt(dimension)),int(np.sqrt(dimension))),\n",
    "                                 event_shape_in=(dimension,)))\n",
    "    bijector = tfb.Chain(bijectors=list(reversed(bijectors)))\n",
    "    \n",
    "    masked_auto_flow = tfd.TransformedDistribution(distribution=base_dist, bijector=bijector)\n",
    "    masked_auto_flow.log_prob(tf.reshape(base_dist.sample(), (28, 28)))\n",
    "    for theta in masked_auto_flow.trainable_variables:\n",
    "        params += np.prod(theta.shape)\n",
    "    print(\"trainable parameters:\", params)\n",
    "    return masked_auto_flow, base_dist, bijectors, bijector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter festlegen und einen Namen für die Checkpoints festlegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.638446Z",
     "start_time": "2022-01-28T13:03:17.624256Z"
    },
    "id": "pbMgBeItYUO_"
   },
   "outputs": [],
   "source": [
    "dataset = \"mnist_all\"\n",
    "layers = 10\n",
    "base_lr = 1e-3\n",
    "end_lr = 1e-4\n",
    "epochs = 80\n",
    "mnist_trainsize = 50000\n",
    "dimension = 784"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modell initialisieren. In diesem Stadium entspricht MAF der Startverteilung bzw. full_bijector der Identitätsabbildung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.940230Z",
     "start_time": "2022-01-28T13:03:17.640214Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MAF, base_dist, list_of_bijectors, full_bijector = AutoregressiveFlow(dimension, layers, inverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.956148Z",
     "start_time": "2022-01-28T13:03:17.942186Z"
    },
    "id": "NhHsL9Y6YUPD"
   },
   "outputs": [],
   "source": [
    "learning_rate = tf.keras.optimizers.schedules.PolynomialDecay(base_lr, epochs, end_lr, power=0.5)\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoints initialisieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.972029Z",
     "start_time": "2022-01-28T13:03:17.958140Z"
    },
    "id": "ntLhLccLYUPE"
   },
   "outputs": [],
   "source": [
    "ckpt_dir = f\"{dataset}/tmp_{layers}\"\n",
    "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt\")\n",
    "\n",
    "ckpt = tf.train.Checkpoint(optimizer=opt, model=MAF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion, die ein Modell trainiert.\n",
    "#### Dabei werden Trainings- und Validierungsdaten verwendet, um Overfitting festzustellen. Nach dem Durchlaufen aller Epochen, wird die benötigte Zeit ausgegeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:03:17.987623Z",
     "start_time": "2022-01-28T13:03:17.975032Z"
    }
   },
   "outputs": [],
   "source": [
    "def TrainFlow(flow, batched_train, batched_val, epochs, train_size, optimizer, checkpoint, checkpoint_pref):\n",
    "\n",
    "    t_losses, v_losses = [], []\n",
    "    t_start = time.time()\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        batched_train.shuffle(buffer_size=train_size, reshuffle_each_iteration=True)\n",
    "        batch_t_losses = []\n",
    "        for batch in batched_train:\n",
    "            batch_loss = train_density_estimation(flow, optimizer, batch)\n",
    "            batch_t_losses.append(batch_loss)\n",
    "        t_loss = tf.reduce_mean(batch_t_losses)\n",
    "\n",
    "        batch_v_losses = []\n",
    "        for batch in batched_val:\n",
    "            batch_loss = nll(flow, batch)\n",
    "            batch_v_losses.append(batch_loss)\n",
    "        v_loss = tf.reduce_mean(batch_v_losses)\n",
    "\n",
    "        t_losses.append(t_loss)\n",
    "        v_losses.append(v_loss)\n",
    "        print(f\"Epoch {i+1}: train loss: {t_loss}, val loss: {v_loss}\")\n",
    "        \n",
    "        if i == 0:\n",
    "            min_v_loss = v_loss\n",
    "            best_epoch = 0\n",
    "        if v_loss < min_v_loss:\n",
    "            min_v_loss = v_loss\n",
    "            best_epoch = i\n",
    "            checkpoint.write(file_prefix=checkpoint_pref)\n",
    "                \n",
    "    print(\"train time:\", time.time() - t_start)\n",
    "    \n",
    "    return t_losses, v_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:04:40.145329Z",
     "start_time": "2022-01-28T13:03:17.988623Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_losses, val_losses = TrainFlow(MAF, batched_train, batched_val, \n",
    "                                     epochs, mnist_trainsize, opt, ckpt, ckpt_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot der Verluste während des Trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:04:40.286701Z",
     "start_time": "2022-01-28T13:04:40.146332Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Y4N94IPTYUPG",
    "outputId": "e4ebed70-e806-4adf-d982-794f60eeb01e"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(train_losses)), train_losses, label=\"train loss\")\n",
    "plt.plot(range(len(val_losses)), val_losses, label=\"val loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laden des Stadiums des Modelles mit geringstem Verlust auf den Validierungsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:04:40.429935Z",
     "start_time": "2022-01-28T13:04:40.288691Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt.restore(ckpt_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion, die den Hintergrund herausfiltert und die Helligkeit erhöht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:04:40.445303Z",
     "start_time": "2022-01-28T13:04:40.431941Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def FilterBackroundPlot(sample, name=\"empty\"):\n",
    "    s = np.array(sample)\n",
    "    s = s- np.median(s)\n",
    "    s = np.abs(s)\n",
    "    s = s/np.max(s)\n",
    "    s = 255*s\n",
    "    s = s.astype(int)\n",
    "    s = np.reshape(s, 784)\n",
    "    s = s*(3)\n",
    "    s = np.where(s>255, 255, s)\n",
    "    s = np.reshape(s, (28, 28))\n",
    "    fig = plt.figure()\n",
    "    plt.imshow(s, cmap=\"gray\")\n",
    "    if name != \"empty\":\n",
    "        plt.savefig(name + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neue Daten generieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:04:52.112238Z",
     "start_time": "2022-01-28T13:04:40.447045Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 5\n",
    "samples = MAF.sample(n)\n",
    "for i in range(n):\n",
    "    FilterBackroundPlot(samples[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funktion, die zweischen zwei Datenpunkten im latenten Raum linear Interpoliert.\n",
    "#### Jedes 28x28 Pixel Bild kann genutzt werden. Je besser das Modell, desto realistischer sind die Zwischenschritte (bzw. desto weniger verblassen/erblassen die Datenpunkte einfach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:04:52.127421Z",
     "start_time": "2022-01-28T13:04:52.114208Z"
    }
   },
   "outputs": [],
   "source": [
    "def LatentInterpolation(start_point, end_point, bijector, epsilon=1/10, name=\"empty\"):\n",
    "    inverse = tfb.Invert(bijector)\n",
    "    start = inverse.forward(start_point)\n",
    "    end = inverse.forward(end_point)\n",
    "    p = start\n",
    "    plt.figure()\n",
    "    FilterBackroundPlot(bijector.forward(p), name=name)\n",
    "    for i in range(int(1/epsilon)):\n",
    "        p += epsilon*(end-start)\n",
    "        if name != \"empty\":\n",
    "            name += str(i)\n",
    "        FilterBackroundPlot(bijector.forward(p), name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:06:33.963837Z",
     "start_time": "2022-01-28T13:04:52.129164Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_start = next(iter(batched_train))[0]\n",
    "real_end = next(iter(batched_train))[0]\n",
    "\n",
    "LatentInterpolation(real_start, real_end, full_bijector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Veranschaulicht die Transformation von Rauschen zu Ziffer schrittweise. \n",
    "#### Alle 3 Schritte werden, durch die Permutation, die ersten 14 mit den letzten 14 Zeilen vertauscht.  Zwischenschritte mit vertauschten Blöcken nicht darzustellen würde aber auch autoregressive Schritte nicht zeigen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:06:33.979573Z",
     "start_time": "2022-01-28T13:06:33.965794Z"
    }
   },
   "outputs": [],
   "source": [
    "def FlowStepsMNIST(latent_point, bijectors_list, name=\"empty\"):\n",
    "    point = latent_point\n",
    "    FilterBackroundPlot(point, name=name)\n",
    "    counter = 1\n",
    "    for bijector in bijectors_list:\n",
    "        point = bijector.forward(point)\n",
    "        if name != \"empty\":\n",
    "            FilterBackroundPlot(point, name=name+str(counter))\n",
    "        else:\n",
    "            FilterBackroundPlot(point, name=name)\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:06:34.879724Z",
     "start_time": "2022-01-28T13:06:33.981570Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "FlowStepsMNIST(full_bijector.inverse(next(iter(batched_train))[0]), list_of_bijectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vergleich der benötigten Zeit für die Vorwärts- bzw Rückwärtstransformation.\n",
    "#### Hier muss full_bijector ineffizient genutzt werden (Vgl. nächste Zelle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:11:41.789556Z",
     "start_time": "2022-01-28T13:06:34.881719Z"
    }
   },
   "outputs": [],
   "source": [
    "latent = base_dist.sample(30)\n",
    "real = next(iter(batched_train))[:30]\n",
    "\n",
    "time_s = time.time()\n",
    "for point in real:\n",
    "    full_bijector.inverse(point)\n",
    "time_e = time.time() - time_s\n",
    "print(\"av_inverse_time:\", time_e/30)\n",
    "\n",
    "time_s = time.time()\n",
    "for point in latent:\n",
    "    full_bijector.forward(point)\n",
    "time_e = time.time() - time_s\n",
    "print(\"av_forward_time:\", time_e/30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Durchschnittlich benötigte Zeit zum Generieren einer Stichprobe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-01-28T13:12:01.303806Z",
     "start_time": "2022-01-28T13:11:41.791539Z"
    }
   },
   "outputs": [],
   "source": [
    "time_s = time.time()\n",
    "MAF.sample(50)\n",
    "av_sample_time = (time.time() -time_s)/50\n",
    "print(av_sample_time)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "maf_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
